{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "rjWAcfmDJFfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GDAL\n",
        "Python\n",
        "Tesnorflow-GPU\n",
        "Get list of dpendcies (Clone VE)\n",
        "explain helpers\n",
        "explain VE activation\n",
        "explain runnung python commends"
      ],
      "metadata": {
        "id": "s0RDdSUwJmXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tiles**"
      ],
      "metadata": {
        "id": "v2AJtuc-JPzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tiles are created using GDAL.\n",
        "The following GDAL codes consist of 2 parts:\n",
        "1. **Combining the RGB raster and the labels raster into one combined raster**. This is the most efficient way, however, you can divide the RGB raster and the labels raster into tiles separately, and then combine them later on in the process (Using a script I will attach later on), although as I said, creating the raster in this step is the preferred way to do it.\n",
        "\n",
        "2. **Generate tiles from the combined images.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mBULFa9bJfp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Combining The Rasters**"
      ],
      "metadata": {
        "id": "4d84EPpmLCzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following commands in terminal (We used Annconda):\n",
        "1. Change the directoy to your working environment:\n",
        "\n",
        "  `cd /home/michael/downloads/generalization/`\n",
        "2. Process all 3 bands of the RGB image (If necessary, Change `image.tif` to the name of the RGB image you want to process):\n",
        "\n",
        "  a. `gdal_translate -b 1 -mask 4 -co COMPRESS=LZW -co BIGTIFF=YES --config GDAL_TIFF_INTERNAL_MASK YES image.tif image_band1.tif`\n",
        "\n",
        "  b.  ` gdal_translate -b 2 -mask 4 -co COMPRESS=LZW -co BIGTIFF=YES --config GDAL_TIFF_INTERNAL_MASK YES image.tif image_band2.tif`\n",
        "\n",
        "  c.  ` gdal_translate -b 3 -mask 4 -co COMPRESS=LZW -co BIGTIFF=YES --config GDAL_TIFF_INTERNAL_MASK YES image.tif image_band3.tif`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Generate a virtual raster of the combined RGB bands and lables band (If necessary, Change `labels.tif` to the name of the labels image you want to process)\n",
        " `gdalbuildvrt -separate combined.vrt image_band1.tif image_band2.tif image_band3.tif labels.tif`\n",
        "\n",
        "4.  Process the virtual raster to an actual tif file:\n",
        "`gdal_translate -co COMPRESS=LZW -co BIGTIFF=YES --config GDAL_TIFF_INTERNAL_MASK YES combined.vrt combined.tif`\n"
      ],
      "metadata": {
        "id": "gDYfn0DWJveu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate Tiles**"
      ],
      "metadata": {
        "id": "SfPtd5sULapZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the parameters and working envorimmnet (if you haven't done it in the last part) and **only then** run the following commend in terminal (again, we used Annconda):\n",
        "\n",
        "`python C:\\ProgramData\\Miniconda\\pkgs\\gdal-3.4.1-py39h9b7a543_0\\Scripts\\gdal_retile.py -co COMPRESS=LZW -co \"TILED=YES\" -ps 256 256 -overlap 5 -targetDir tiles_no_overlap -tileIndex index combined.tif`\n",
        "\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "\n",
        "*   `-ps` = Number of pixels in each tile\n",
        "*   `-overlap` = Pixel overlap of the tiles. For example- for an overlap of 50% of a 256x256 tile with the previous 256x256 tile, the value of this parameter should be 128 (256/2).\n",
        "*   `-targetDir` = The directory the tiles will be saved at. Make sure this directory exists **before** you run the code.\n",
        "*   `-tileIndex index` = The raster you want to divide.\n",
        "\n",
        "For more information about the parmaters of this script go to https://gdal.org/programs/gdal_retile.html.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j2nXzmH5L30h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "UqBtAOeVvR4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helpers**"
      ],
      "metadata": {
        "id": "NZeQqhXNMIbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers class should be in the same working directory as the python code.\n",
        "There is no need to adjust or run the helpers code."
      ],
      "metadata": {
        "id": "jwzc7tTPMNC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "\n",
        "## Read image into tuple of RGB [x, y, 1-3] and target [x, y, 4]\n",
        "def read_image(path):\n",
        "    src = rasterio.open(path)\n",
        "    r = src.read()\n",
        "    r[3][r[3] == 255] = 0  ## Replace 255 with 0 in 4th band\n",
        "    r = np.moveaxis(r, 0, 2)\n",
        "    image = r[:, :, :3]\n",
        "    target = r[:, :, [3]]\n",
        "    return (image, target)\n",
        "\n",
        "## Read images into two tuples with arrays [i, x, y, 1-3] and [i, x, y, 4]\n",
        "def read_images(paths):\n",
        "    images = [read_image(i) for i in paths]\n",
        "    images, targets = zip(*images)\n",
        "    images = np.stack(images)\n",
        "    targets = np.stack(targets)\n",
        "    return (images, targets)\n",
        "\n",
        "## Predict raster (file_in=RGB, file_out=pred:0,1,2)\n",
        "def predict_raster(model, filename_in, filename_out, size):\n",
        "    src = rasterio.open(filename_in)\n",
        "    r = read_image(filename_in)[0]\n",
        "    if r.shape != size:\n",
        "        return 0\n",
        "    if (r.flatten() == 0).all():\n",
        "        return 0\n",
        "    mask = model.predict(np.expand_dims(r, 0), verbose=0)[0]\n",
        "    pred = np.argmax(mask, axis=-1)\n",
        "    meta = src.meta\n",
        "    meta.update(count=1)\n",
        "    dst = rasterio.open(filename_out, 'w', **meta)\n",
        "    dst.write(pred, 1)\n",
        "    dst.close()\n",
        "    src.close()\n"
      ],
      "metadata": {
        "id": "NWgA9H7KMqrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filter (Optional)**"
      ],
      "metadata": {
        "id": "CzkawqHN1y-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The folllwoing code filters the tiles dataset and keeps only images that contain at least one pixes of a solar panel.\n"
      ],
      "metadata": {
        "id": "CdxvJDofwNGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Paths\n",
        "path_in = 'D:/U-Net/training/combined_images/'\n",
        "path_out = 'D:/U-Net/Segnet/model_input/'\n",
        "\n",
        "# Minimum number of class=2 (solar panel) pixels\n",
        "cutoff = 12\n",
        "\n",
        "# Files Load\n",
        "files = glob.glob(path_in + '*.tif')\n",
        "\n",
        "# Filter\n",
        "for i in files:\n",
        "  src = rasterio.open(i)\n",
        "  r = src.read()\n",
        "  if (r[3] > 0).sum() > cutoff:\n",
        "    shutil.copy(i, path_out)"
      ],
      "metadata": {
        "id": "Q3CAXiTm2ZZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate Train, Test & Validation Datasets**\n"
      ],
      "metadata": {
        "id": "tdKJ6lAzvzbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No need to edit, except relevant paths."
      ],
      "metadata": {
        "id": "-Go6Y3LtXKCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "import glob\n",
        "import random\n",
        "\n",
        "# Input Path- enter the path of the directory that contains the tiles.\n",
        "path = 'D:/U-Net/Segnet/model_input/'\n",
        "\n",
        "# Loads & Sorts Files\n",
        "files = glob.glob(path + '*.tif')\n",
        "files.sort()\n",
        "\n",
        "# Shuffles the order of the tiles\n",
        "random.Random(7).shuffle(files)\n",
        "\n",
        "## DS Split to Train, Test & Validation\n",
        "# Cutoffs\n",
        "cutoff1 = int(len(files)*0.8) # 80% Training\n",
        "cutoff2 = cutoff1 + int(len(files)*0.1) # 10% Test & 10% Validation.\n",
        "# Slicing\n",
        "files_training = files[:cutoff1]\n",
        "files_validation = files[cutoff1:cutoff2]\n",
        "files_test = files[cutoff2:]\n",
        "\n",
        "\n",
        "## Save- If you are retraining change the name of the txt files or the output path\n",
        "# Path to save the results\n",
        "Output_path = \"D:/U-Net/Segnet/results/result_files_\"\n",
        "\n",
        "# Train\n",
        "file = open(Output_path + \"training.txt\", 'w')\n",
        "file.writelines('\\n'.join(files_training))\n",
        "file.close()\n",
        "# Test\n",
        "file = open(Output_path + \"validation.txt\", 'w')\n",
        "file.writelines('\\n'.join(files_validation))\n",
        "file.close()\n",
        "# Validation\n",
        "file = open(Output_path + \"test.txt\", 'w')\n",
        "file.writelines('\\n'.join(files_test))\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "aB8TTfc9wnQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "8qwgfQag3pHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Segnet architecture**"
      ],
      "metadata": {
        "id": "v_9yoLteUHIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "import glob\n",
        "import random\n",
        "import helpers\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (4, 4)\n",
        "\n",
        "# Read training files using path (path can be edited)\n",
        "\n",
        "files = open('D:/U-Net/Segnet/results/result_21_files_training.txt').read().splitlines()\n",
        "\n",
        "training_images, training_targets = helpers.read_images(files)\n",
        "print(\"training images shape:\", training_images.shape)\n",
        "print(\"raining targets shape:\", training_targets.shape)\n",
        "\n",
        "# Read validation using path (path can be edited)\n",
        "files = open('D:/U-Net/Segnet/results/result_21_files_validation.txt').read().splitlines()\n",
        "\n",
        "validation_images, validation_targets = helpers.read_images(files)\n",
        "\n",
        "# Segnet arichetecture  - optimized for 256X256 tiles.\n",
        "\n",
        "img_size = training_images[0,:,:,0].shape\n",
        "def get_model(img_size, num_classes):\n",
        "    # Encoder\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    outputs = layers.Conv2D(num_classes, (3, 3), padding='same', activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = get_model(img_size=img_size, num_classes=3)\n",
        "model.summary() #print model summary\n",
        "\n",
        "# Compile model using rmsprop and loss sparse categorical crossentropy, can be changed according to specific needs\n",
        "# Also epochs number, callback and batch size can be edited according to the relevant mission\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('solar_panels.keras', save_best_only=True)]\n",
        "history = model.fit(\n",
        "    training_images, training_targets,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    batch_size=64,\n",
        "    validation_data=(validation_images, validation_targets)\n",
        ")\n",
        "\n",
        "# Save Model to specific path so we can load it after\n",
        "model.save('D:/U-Net/Segnet/models/model_segnet')\n",
        "\n",
        "# Plot Model Fitting History to check the performence\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.savefig('D:/U-Net/Segnet/history_20Epochs.png')"
      ],
      "metadata": {
        "id": "iBOKnTii3vDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **UNET**"
      ],
      "metadata": {
        "id": "-kGwtIyrQN3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import glob\n",
        "import random\n",
        "import helpers\n",
        "\n",
        "## Settings for running Tensorflow with GPU\n",
        "\n",
        "print(tf.test.is_gpu_available())\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
        "\n",
        "# Get the list of physical devices\n",
        "physical_devices = tf.config.list_physical_devices()\n",
        "\n",
        "# Print the list of physical devices\n",
        "print(physical_devices)\n",
        "\n",
        "# Check if GPU is in the list of physical devices\n",
        "if len(physical_devices) > 0:\n",
        "    for device in physical_devices:\n",
        "        if device.device_type == 'GPU':\n",
        "            print('GPU is set as the default device.')\n",
        "            break\n",
        "else:\n",
        "    print('No GPU is available.')\n",
        "\n",
        "### U-Net\n",
        "\n",
        "# Defining inputs:\n",
        "\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "\n",
        "## Build the U-Net model architecture to fit our input size 256X256\n",
        "\n",
        "\n",
        "# Load inputs and normalize\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "\n",
        "# Contraction path\n",
        "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
        "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "p5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c5)\n",
        "\n",
        "c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p5)\n",
        "c6 = tf.keras.layers.Dropout(0.3)(c6)\n",
        "c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c6)\n",
        "\n",
        "# Expansive path\n",
        "u7 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c5])\n",
        "c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "u8 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c4])\n",
        "c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = tf.keras.layers.Dropout(0.2)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "u9 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c3])\n",
        "c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = tf.keras.layers.Dropout(0.2)(c9)\n",
        "c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "u10 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c9)\n",
        "u10 = tf.keras.layers.concatenate([u10, c2])\n",
        "c10 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u10)\n",
        "c10 = tf.keras.layers.Dropout(0.1)(c10)\n",
        "c10 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c10)\n",
        "\n",
        "u11 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c10)\n",
        "u11 = tf.keras.layers.concatenate([u11, c1], axis=3)\n",
        "c11 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u11)\n",
        "c11 = tf.keras.layers.Dropout(0.1)(c11)\n",
        "c11 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c11)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid')(c11)\n",
        "\n",
        "# Setting model inputs & outputs\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "# Printing model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read training data from path\n",
        "files = open('D:/U-Net/Segnet/results/result_21_files_training.txt').read().splitlines()\n",
        "\n",
        "# Open data using helpers\n",
        "training_images, training_targets = helpers.read_images(files)\n",
        "\n",
        "# Print shapes\n",
        "print(\"training images shape:\", training_images.shape)\n",
        "print(\"training targets shape:\", training_targets.shape)\n",
        "\n",
        "# Read validation data from path\n",
        "files = open('D:/U-Net/Segnet/results/result_21_files_validation.txt').read().splitlines()\n",
        "\n",
        "# Open data using helpers\n",
        "validation_images, validation_targets = helpers.read_images(files)\n",
        "\n",
        "# Compile model using rmsprop and loss sparse categorical crossentropy, can be changed according to specific needs\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Setting callbacks\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('solar_panels.keras', save_best_only=True)]\n",
        "\n",
        "# Train the model\n",
        "# Also epochs number, callback and batch size can be edited according to the relevant mission\n",
        "\n",
        "history = model.fit(\n",
        "    training_images, training_targets,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    batch_size=64,\n",
        "    validation_data=(validation_images, validation_targets)\n",
        ")\n",
        "\n",
        "# Save Model to specific path so we can load it after\n",
        "model.save('D:/U-Net/U-Net_GPU')\n",
        "\n",
        "# Plot Model Fitting History to check the performence is an option as in Segnet, we decided that it's not relevant in that case."
      ],
      "metadata": {
        "id": "viaNBd_eQLig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**"
      ],
      "metadata": {
        "id": "5P_SlNV02mJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "import glob\n",
        "import helpers\n",
        "import sklearn.metrics\n",
        "\n",
        "# Load Model from the path that we saved into\n",
        "model = keras.models.load_model('D:/U-Net/Segnet/models/model_segnet_v01')\n",
        "\n",
        "# Read the test files, path can be edited\n",
        "files = open('D:/U-Net/Segnet/results/result_21_files_test.txt').read().splitlines()\n",
        "\n",
        "test_images, test_targets = helpers.read_images(files)\n",
        "\n",
        "# Preidct Test\n",
        "mask = model.predict(test_images)\n",
        "pred = np.argmax(mask, axis=-1)\n",
        "np.unique(pred.flatten())\n",
        "obs = test_targets[:, :, :, 0]\n",
        "y_obs = obs.flatten()\n",
        "y_pred = pred.flatten()\n",
        "\n",
        "# Confution Matrix\n",
        "conf_matrix = sklearn.metrics.confusion_matrix(y_obs, y_pred, labels=[0, 1, 2])\n",
        "\n",
        "# Define the headers and indices\n",
        "headers = [\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"]\n",
        "indices = [\"Actual 0\", \"Actual 1\", \"Actual 2\"]\n",
        "\n",
        "# Print the confusion matrix with headers and indices\n",
        "print(\"\\t\\t\" + \"\\t\".join(headers))\n",
        "for i in range(len(conf_matrix)):\n",
        "    row_str = \"\\t\".join([str(x) for x in conf_matrix[i]])\n",
        "    print(indices[i] + \"\\t\" + row_str)\n",
        "\n",
        "# Calculate true positives (TP), false positives (FP), and false negatives (FN) for each class\n",
        "TP = np.diag(conf_matrix)\n",
        "FP = np.sum(conf_matrix, axis=0) - TP\n",
        "FN = np.sum(conf_matrix, axis=1) - TP\n",
        "\n",
        "# Calculate precision and recall for each class\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Print precision and recall for each class\n",
        "for i in range(len(precision)):\n",
        "    print(\"Class\", i)\n",
        "    print(\"Precision:\", precision[i])\n",
        "    print(\"Recall:\", recall[i])\n",
        "    print(\"F1 score:\", f1_score[i])\n",
        "    print()"
      ],
      "metadata": {
        "id": "pyPuvWet3Glx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generalization**"
      ],
      "metadata": {
        "id": "u66rwrqwLqiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Creation**"
      ],
      "metadata": {
        "id": "IMW5yOoWM5Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as in training but only test set.\n",
        "Path can be changed according to your paths.\n",
        "\n"
      ],
      "metadata": {
        "id": "3QPXjedrNCvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "import glob\n",
        "import random\n",
        "\n",
        "path = 'D:/U-Net/generalization/tiles_no_overlap/'\n",
        "\n",
        "# Load Files\n",
        "files = glob.glob(path + '*.tif')\n",
        "filtered_files = []\n",
        "\n",
        "for f in files:\n",
        "    with rasterio.open(f) as src:\n",
        "        height, width = src.shape\n",
        "        if height == 256 and width == 256:\n",
        "            filtered_files.append(f)\n",
        "\n",
        "print(\"Number Of Files:\", len(filtered_files))\n",
        "\n",
        "# Write\n",
        "file = open('D:/U-Net/Segnet/Gen/results/Gen_tiles_no_overlap_dim.txt', 'w')\n",
        "file.writelines('\\n'.join(filtered_files))\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "xXsJkhhrNB9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**"
      ],
      "metadata": {
        "id": "w5EkMG_qNg2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "same but runs in batches due to size"
      ],
      "metadata": {
        "id": "d48zwjKaNyqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import glob\n",
        "import helpers\n",
        "import sklearn.metrics\n",
        "\n",
        "# Load Model\n",
        "#model = keras.models.load_model('D:/U-Net/Segnet/models/model_segnet_v01')\n",
        "model = keras.models.load_model('D:/U-Net/Segnet/models/model_segnet_v06_half_20Epochs')\n",
        "\n",
        "# Read file paths\n",
        "files = open('D:/U-Net/Segnet/Gen/results/Gen_tiles_no_overlap_dim.txt').read().splitlines()\n",
        "\n",
        "# Initialize the cumulative confusion matrix\n",
        "conf_matrix_cumulative = np.zeros((3, 3))\n",
        "\n",
        "# Process images in batches, 500 were chosen due to memory considerations.\n",
        "batch_size = 500\n",
        "num_batches = len(files) // batch_size\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    batch_files = files[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
        "    print(\"Batch\", batch_idx, \"Start\")\n",
        "    # Read batch of images\n",
        "    batch_images, batch_targets = helpers.read_images(batch_files)\n",
        "    print(\"Load Done\")\n",
        "\n",
        "    # Predict batch\n",
        "    batch_mask = model.predict(batch_images)\n",
        "    print(\"Mask Done\")\n",
        "    batch_pred = np.argmax(batch_mask, axis=-1)\n",
        "    print(\"Pred Done\")\n",
        "    batch_obs = batch_targets[:, :, :, 0]\n",
        "    print(\"Obs Done\")\n",
        "\n",
        "    # Calculate batch confusion matrix\n",
        "    conf_matrix = sklearn.metrics.confusion_matrix(batch_obs.flatten(), batch_pred.flatten(), labels=[0, 1, 2])\n",
        "\n",
        "    # Add batch confusion matrix to the cumulative confusion matrix\n",
        "    conf_matrix_cumulative += conf_matrix\n",
        "\n",
        "    print(\"Batch\", batch_idx, \"completed\")\n",
        "\n",
        "# Define the headers and indices\n",
        "headers = [\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"]\n",
        "indices = [\"Actual 0\", \"Actual 1\", \"Actual 2\"]\n",
        "\n",
        "# Print the cumulative confusion matrix with headers and indices\n",
        "print(\"\\t\\t\" + \"\\t\".join(headers))\n",
        "for i in range(len(conf_matrix_cumulative)):\n",
        "    row_str = \"\\t\".join([str(x) for x in conf_matrix_cumulative[i]])\n",
        "    print(indices[i] + \"\\t\" + row_str)\n",
        "\n",
        "# Calculate true positives (TP), false positives (FP), and false negatives (FN) for each class\n",
        "TP = np.diag(conf_matrix_cumulative)\n",
        "FP = np.sum(conf_matrix_cumulative, axis=0) - TP\n",
        "FN = np.sum(conf_matrix_cumulative, axis=1) - TP\n",
        "\n",
        "# Calculate precision and recall for each class\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Print precision and recall for each class\n",
        "for i in range(len(precision)):\n",
        "    print(\"Class\", i)\n",
        "    print(\"Precision:\", precision[i])\n",
        "    print(\"Recall:\", recall[i])\n",
        "    print(\"F1 score:\", f1_score[i])\n",
        "    print()\n",
        "\n",
        "# # Calculate overall TP, FP, and FN for all classes\n",
        "# overall_TP = np.sum(TP)\n",
        "# overall_FP = np.sum(FP)\n",
        "# overall_FN = np.sum(FN)\n",
        "\n",
        "# # Calculate overall precision and recall\n",
        "# overall_precision = overall_TP / (overall_TP + overall_FP)\n",
        "# overall_recall = overall_TP / (overall_TP + overall_FN)\n",
        "# overall_f1_score = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall)\n",
        "\n",
        "# # Print overall precision and recall\n",
        "# print(\"Overall Precision:\", overall_precision)\n",
        "# print(\"Overall Recall:\", overall_recall)\n",
        "# print(\"Overall F1 score:\", overall_f1_score)"
      ],
      "metadata": {
        "id": "DRxbgG_CNlxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Additonal Code**"
      ],
      "metadata": {
        "id": "_L01pp4nN4xO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generting Images**"
      ],
      "metadata": {
        "id": "HEw6ayrlOykd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "import glob\n",
        "import helpers\n",
        "import sklearn.metrics\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (4, 4)\n",
        "\n",
        "model = keras.models.load_model('D:/U-Net/Segnet/models/model_segnet_v01')\n",
        "\n",
        "# Read\n",
        "files = open('D:/U-Net/Segnet/results/result_21_files_test.txt').read().splitlines()\n",
        "test_images, test_targets = helpers.read_images(files)"
      ],
      "metadata": {
        "id": "ubwAygrEO9tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For One Exemple\n",
        "i = 6\n",
        "image = test_images[i]\n",
        "plt.imshow(image)\n",
        "#plt.show()\n",
        "plt.savefig('D:/U-Net/Segnet/visual_comp/test.png')\n",
        "\n",
        "target = test_targets[i]\n",
        "plt.imshow(target)\n",
        "#plt.show()\n",
        "plt.savefig('D:/U-Net/Segnet/visual_comp/target.png')\n",
        "\n",
        "np.expand_dims(image, 0).shape\n",
        "mask = model.predict(np.expand_dims(image, 0))[0]\n",
        "pred = np.argmax(mask, axis=-1)\n",
        "np.unique(pred.flatten())\n",
        "plt.imshow(pred)\n",
        "#plt.show()\n",
        "plt.savefig('D:/U-Net/Segnet/visual_comp/pred.png')\n",
        "\n",
        "mask = model.predict(test_images)\n",
        "pred = np.argmax(mask, axis=-1)\n",
        "np.unique(pred.flatten())\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "qfiVTEzmO5w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in  range(len(test_images)):\n",
        "    # RGB\n",
        "    image = test_images[i]\n",
        "    plt.imshow(image)\n",
        "    plt.savefig('D:/U-Net/Segnet/visual_comp/test/test['+ str(i) +'].png')\n",
        "    # Target\n",
        "    target = test_targets[i]\n",
        "    plt.imshow(target)\n",
        "    plt.savefig('D:/U-Net/Segnet/visual_comp/target/target[' + str(i) + '].png')\n",
        "    # Pred\n",
        "    np.expand_dims(image, 0).shape\n",
        "    mask = model.predict(np.expand_dims(image, 0))[0]\n",
        "    pred = np.argmax(mask, axis=-1)\n",
        "    np.unique(pred.flatten())\n",
        "    plt.imshow(pred)\n",
        "    plt.savefig('D:/U-Net/Segnet/visual_comp/pred/pred[' + str(i) + '].png')\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "dFXhro6YPDje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}